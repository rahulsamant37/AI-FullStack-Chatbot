{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/AI-FullStack-Chatbot/notebook'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/AI-FullStack-Chatbot'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data_file: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath= CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH,\n",
    "                 schema_filepath = SCHEMA_FILE_PATH):\n",
    "        self.config=read_yaml(config_filepath)\n",
    "        self.params=read_yaml(params_filepath)\n",
    "        self.schema=read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_ingestion_config(self)-> DataIngestionConfig:\n",
    "        config=self.config.data_ingestion\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config=DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file\n",
    "\n",
    "        )\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from src import logger\n",
    "from typing import List, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestionError(Exception):\n",
    "    \"\"\"Custom exception for data ingestion errors\"\"\"\n",
    "    pass\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "        self.companies: List[Dict[str, Any]] = []\n",
    "        self.logs_dir = os.path.join(self.config.root_dir, \"logs\")\n",
    "        self.output_dir = os.path.join(self.config.root_dir, \"raw\")\n",
    "        self.semaphore = asyncio.Semaphore(4)\n",
    "        self.logger = logger\n",
    "        \n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    async def initiate_data_ingestion(self) -> Optional[str]:\n",
    "        try:\n",
    "            # Initialize playwright\n",
    "            self.logger.info(\"Initializing playwright...\")\n",
    "            playwright = await async_playwright().start()\n",
    "            if not playwright:\n",
    "                raise DataIngestionError(\"Failed to initialize playwright\")\n",
    "            \n",
    "            self.logger.info(\"Launching browser...\")\n",
    "            browser = await playwright.chromium.launch(\n",
    "                headless=True,\n",
    "                args=[\n",
    "                    '--no-sandbox',\n",
    "                    '--disable-setuid-sandbox',\n",
    "                    '--disable-dev-shm-usage',\n",
    "                    '--disable-gpu'\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            if not browser:\n",
    "                raise DataIngestionError(\"Failed to launch browser\")\n",
    "            self.logger.info(\"Browser launched successfully\")\n",
    "\n",
    "            try:\n",
    "                self.logger.info(\"Creating browser context...\")\n",
    "                context = await browser.new_context(\n",
    "                    user_agent=random.choice(USER_AGENTS),\n",
    "                    viewport={'width': 1920, 'height': 1080},\n",
    "                    ignore_https_errors=True\n",
    "                )\n",
    "                \n",
    "                if not context:\n",
    "                    raise DataIngestionError(\"Failed to create browser context\")\n",
    "                self.logger.info(\"Browser context created successfully\")\n",
    "\n",
    "                try:\n",
    "                    self.logger.info(\"Creating new page...\")\n",
    "                    page = await context.new_page()\n",
    "                    \n",
    "                    if not page:\n",
    "                        raise DataIngestionError(\"Failed to create new page\")\n",
    "                    self.logger.info(\"New page created successfully\")\n",
    "\n",
    "                    # Set timeout after verifying page exists\n",
    "                    self.logger.info(\"Setting page timeout...\")\n",
    "                    page.set_default_timeout(30000)  # Removed await as it's not an async method\n",
    "                    \n",
    "                    self.logger.info(\"Starting company listings scrape...\")\n",
    "                    await self.scrape_companies(page)\n",
    "                    \n",
    "                    if not self.companies:\n",
    "                        raise DataIngestionError(\"No companies found during scraping\")\n",
    "                    \n",
    "                    self.logger.info(f\"Found {len(self.companies)} companies\")\n",
    "                    self.logger.info(\"Starting parallel job scraping...\")\n",
    "                    \n",
    "                    tasks = [self.process_company(browser, company) \n",
    "                            for company in self.companies]\n",
    "                    \n",
    "                    await asyncio.gather(*tasks)\n",
    "                    \n",
    "                finally:\n",
    "                    self.logger.info(\"Closing page...\")\n",
    "                    if page:\n",
    "                        await page.close()\n",
    "            \n",
    "            finally:\n",
    "                self.logger.info(\"Closing context...\")\n",
    "                if context:\n",
    "                    await context.close()\n",
    "                \n",
    "                self.logger.info(\"Closing browser...\")\n",
    "                if browser:\n",
    "                    await browser.close()\n",
    "                \n",
    "                self.logger.info(\"Closing playwright...\")\n",
    "                if playwright:\n",
    "                    await playwright.stop()\n",
    "\n",
    "            self.logger.info(\"Data ingestion completed successfully\")\n",
    "            return self.output_dir\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Data ingestion failed: {str(e)}\")\n",
    "            raise DataIngestionError(f\"Data ingestion failed: {str(e)}\") from e\n",
    "\n",
    "    async def scrape_companies(self, page) -> None:\n",
    "        try:\n",
    "            self.logger.info(f\"Navigating to {self.config.source_URL}\")\n",
    "            await page.goto(self.config.source_URL, wait_until='networkidle')\n",
    "            self.logger.info(\"Waiting for company selector...\")\n",
    "            await page.wait_for_selector('.row li.clicky')\n",
    "            \n",
    "            companies = await page.query_selector_all('.row li.clicky')\n",
    "            self.logger.info(f\"Found {len(companies)} companies on the page\")\n",
    "            \n",
    "            for company in companies:\n",
    "                try:\n",
    "                    name = await (await company.query_selector('strong')).inner_text()\n",
    "                    job_count = (await (await company.query_selector('.jobs-count')).inner_text()).split()[0]\n",
    "                    url = await company.get_attribute('data-url')\n",
    "                    \n",
    "                    self.companies.append({\n",
    "                        'name': name.strip(),\n",
    "                        'job_count': int(job_count),\n",
    "                        'url': f\"https://www.careerjet.co.in{url}\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing company element: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Company scraping failed: {str(e)}\")\n",
    "            raise DataIngestionError(f\"Company scraping failed: {str(e)}\") from e\n",
    "\n",
    "    async def process_company(self, browser, company: Dict[str, Any]) -> None:\n",
    "        async with self.semaphore:\n",
    "            company_name = company.get('name', 'Unknown Company')\n",
    "            self.logger.info(f\"Starting job scrape for {company_name}\")\n",
    "            \n",
    "            try:\n",
    "                context = await browser.new_context(\n",
    "                    user_agent=random.choice(USER_AGENTS),\n",
    "                    viewport={'width': 1920, 'height': 1080},\n",
    "                    ignore_https_errors=True\n",
    "                )\n",
    "                \n",
    "                if not context:\n",
    "                    raise DataIngestionError(f\"Failed to create context for {company_name}\")\n",
    "                \n",
    "                page = await context.new_page()\n",
    "                if not page:\n",
    "                    raise DataIngestionError(f\"Failed to create page for {company_name}\")\n",
    "                \n",
    "                page.set_default_timeout(30000)  # Removed await as it's not an async method\n",
    "                \n",
    "                jobs = await self.scrape_company_jobs(page, company)\n",
    "                \n",
    "                if jobs:\n",
    "                    await self.save_company_data(company, jobs)\n",
    "                else:\n",
    "                    self.logger.warning(f\"No jobs found for {company_name}\")\n",
    "                \n",
    "                await context.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing company {company_name}: {str(e)}\")\n",
    "\n",
    "    async def scrape_company_jobs(self, page, company: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        company_jobs = []\n",
    "        max_pages = 4\n",
    "\n",
    "        try:\n",
    "            for page_num in range(1, max_pages + 1):\n",
    "                if page_num == 1:\n",
    "                    await page.goto(company['url'])\n",
    "                else:\n",
    "                    next_button = await page.query_selector('button.ves-control.next')\n",
    "                    if not next_button:\n",
    "                        break\n",
    "                    await next_button.click()\n",
    "                \n",
    "                try:\n",
    "                    await page.wait_for_selector('article.job', timeout=10000)\n",
    "                    await asyncio.sleep(2)\n",
    "                    \n",
    "                    jobs = await page.query_selector_all('article.job')\n",
    "                    if not jobs:\n",
    "                        break\n",
    "                    \n",
    "                    for job in jobs:\n",
    "                        try:\n",
    "                            job_data = await self.extract_job_data(job)\n",
    "                            company_jobs.append(job_data)\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Error extracting job data: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    await asyncio.sleep(2)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error on page {page_num}: {str(e)}\")\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error scraping jobs: {str(e)}\")\n",
    "        \n",
    "        return company_jobs\n",
    "\n",
    "    async def extract_job_data(self, job) -> Dict[str, Any]:\n",
    "        title = await (await job.query_selector('h2 a')).inner_text()\n",
    "        \n",
    "        location_elem = await job.query_selector('.location li')\n",
    "        location = await location_elem.inner_text() if location_elem else \"Not specified\"\n",
    "        \n",
    "        desc_elem = await job.query_selector('.desc')\n",
    "        description = await desc_elem.inner_text() if desc_elem else \"No description available\"\n",
    "        \n",
    "        posted_elem = await job.query_selector('.tags .badge')\n",
    "        posted = await posted_elem.inner_text() if posted_elem else \"Not specified\"\n",
    "        \n",
    "        return {\n",
    "            'title': title.strip(),\n",
    "            'location': location.strip(),\n",
    "            'description': description.strip(),\n",
    "            'posted': posted.strip()\n",
    "        }\n",
    "\n",
    "    async def save_company_data(self, company: Dict[str, Any], jobs: List[Dict[str, Any]]) -> None:\n",
    "        try:\n",
    "            current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            company_name_safe = \"\".join(x for x in company['name'] if x.isalnum() or x in (' ', '-', '_')).strip()\n",
    "            filename = os.path.join(self.output_dir, f\"{company_name_safe}_jobs.json\")\n",
    "\n",
    "            json_data = {\n",
    "                \"company_name\": company['name'],\n",
    "                \"scraped_on\": current_date,\n",
    "                \"total_jobs_available\": company['job_count'],\n",
    "                \"company_url\": company['url'],\n",
    "                \"jobs\": jobs\n",
    "            }\n",
    "\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, indent=4)\n",
    "\n",
    "            self.logger.info(f\"Generated JSON file: {filename}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving data for {company['name']}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-04 13:46:45,325: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-01-04 13:46:45,329: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-01-04 13:46:45,330: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-01-04 13:46:45,330: INFO: common: created directory at: artifacts]\n",
      "[2025-01-04 13:46:45,331: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2025-01-04 13:46:45,332: INFO: 2547267758: Initializing playwright...]\n",
      "[2025-01-04 13:46:45,671: INFO: 2547267758: Launching browser...]\n",
      "[2025-01-04 13:46:45,751: INFO: 2547267758: Browser launched successfully]\n",
      "[2025-01-04 13:46:45,752: INFO: 2547267758: Creating browser context...]\n",
      "[2025-01-04 13:46:45,769: INFO: 2547267758: Browser context created successfully]\n",
      "[2025-01-04 13:46:45,770: INFO: 2547267758: Creating new page...]\n",
      "[2025-01-04 13:46:45,819: INFO: 2547267758: New page created successfully]\n",
      "[2025-01-04 13:46:45,820: INFO: 2547267758: Setting page timeout...]\n",
      "[2025-01-04 13:46:45,827: INFO: 2547267758: Starting company listings scrape...]\n",
      "[2025-01-04 13:46:45,828: INFO: 2547267758: Navigating to https://www.careerjet.co.in/company]\n",
      "[2025-01-04 13:46:48,277: INFO: 2547267758: Waiting for company selector...]\n",
      "[2025-01-04 13:46:48,354: INFO: 2547267758: Found 12 companies on the page]\n",
      "[2025-01-04 13:46:48,985: INFO: 2547267758: Found 12 companies]\n",
      "[2025-01-04 13:46:48,986: INFO: 2547267758: Starting parallel job scraping...]\n",
      "[2025-01-04 13:46:48,987: INFO: 2547267758: Starting job scrape for Ciel HR]\n",
      "[2025-01-04 13:46:48,993: INFO: 2547267758: Starting job scrape for PwC]\n",
      "[2025-01-04 13:46:49,000: INFO: 2547267758: Starting job scrape for IBM]\n",
      "[2025-01-04 13:46:49,006: INFO: 2547267758: Starting job scrape for Wipro]\n",
      "[2025-01-04 13:47:26,703: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/IBM_jobs.json]\n",
      "[2025-01-04 13:47:26,727: INFO: 2547267758: Starting job scrape for TekPillar]\n",
      "[2025-01-04 13:47:26,836: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/Wipro_jobs.json]\n",
      "[2025-01-04 13:47:26,844: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/Ciel HR_jobs.json]\n",
      "[2025-01-04 13:47:26,857: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/PwC_jobs.json]\n",
      "[2025-01-04 13:47:26,898: INFO: 2547267758: Starting job scrape for EY]\n",
      "[2025-01-04 13:47:26,939: INFO: 2547267758: Starting job scrape for Kotak Mahindra Bank]\n",
      "[2025-01-04 13:47:26,954: INFO: 2547267758: Starting job scrape for Oracle]\n",
      "[2025-01-04 13:47:54,330: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/TekPillar_jobs.json]\n",
      "[2025-01-04 13:47:54,353: INFO: 2547267758: Starting job scrape for Genpact]\n",
      "[2025-01-04 13:48:00,795: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/EY_jobs.json]\n",
      "[2025-01-04 13:48:00,814: INFO: 2547267758: Starting job scrape for Equitas Small Finance Bank]\n",
      "[2025-01-04 13:48:01,178: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/Oracle_jobs.json]\n",
      "[2025-01-04 13:48:01,185: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/Kotak Mahindra Bank_jobs.json]\n",
      "[2025-01-04 13:48:01,232: INFO: 2547267758: Starting job scrape for Infosys]\n",
      "[2025-01-04 13:48:01,246: INFO: 2547267758: Starting job scrape for Amazon]\n",
      "[2025-01-04 13:48:27,362: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/Genpact_jobs.json]\n",
      "[2025-01-04 13:48:28,392: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/Equitas Small Finance Bank_jobs.json]\n",
      "[2025-01-04 13:48:33,379: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/Amazon_jobs.json]\n",
      "[2025-01-04 13:48:33,753: INFO: 2547267758: Generated JSON file: artifacts/data_ingestion/raw/Infosys_jobs.json]\n",
      "[2025-01-04 13:48:33,777: INFO: 2547267758: Closing page...]\n",
      "[2025-01-04 13:48:33,798: INFO: 2547267758: Closing context...]\n",
      "[2025-01-04 13:48:33,827: INFO: 2547267758: Closing browser...]\n",
      "[2025-01-04 13:48:33,877: INFO: 2547267758: Closing playwright...]\n",
      "[2025-01-04 13:48:33,889: INFO: 2547267758: Data ingestion completed successfully]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    output_path = await data_ingestion.initiate_data_ingestion()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
